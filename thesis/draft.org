#+title: Bachelor Thesis: Grammatical Evolution - Draft
#+description: Draft for bachelor thesis.
#+author: Markus Fruhmann
#+bibliography: references.bib

* Discovering Regular Grammars and Regular Expressions with Grammatical Evolution
** Abstract
Short summary with conclusion in one sentence in the end
** Introduction
Abstract, but longer
** Related Work
AAlpy
Some Italian working with regular grammars
** Preliminaries
John R. Koza, how does GP work
Definition of used regular grammars
Definition of used regular expressions

*** Definition of Languages
Before talking about creating regular expressions or regular grammars for different languages, we first have to define what a language is.

Each language consists of an alphabet, which is a nonempty, finite set of characters. This alphabet will be noted as \Sigma. Using this alphabet, we are able to construct words. A word is simply a string, a finite sequence of characters from the alphabet. [cite:see @books/PLinz/fla-2017 p. 17]
In our case, all characters of an alphabet are lowercase letters _a_, _b_, _c_,... for elements of \Sigma, since these letters signify the non-terminals in our regular grammars.
Each word \omega of our language has a length, denoted by |\omega|. The length can be obtained by counting all characters that make up a specific word. A very important special word is the empty string \lambda, which does not contain any characters and therefore has a length of 0. [cite:see @books/PLinz/fla-2017 p. 17]
As we now have defined the alphabet \Sigma, a word of the alphabet \omega and the empty string \lambda, we can now look at two special alphabets, \Sigma^{\+} and \Sigma^{\*}. \Sigma^{\*} contains all words that can be created using the characters in \Sigma, including the empty string \lambda. To exclude \lambda, we can define \Sigma^{\+} as follows:
\begin{equation}
\Sigma^{\+}=\Sigma^{\*} - {\lambda}.
\end{equation}
In contrast to the finite nature of \Sigma, \Sigma^{\*} and \Sigma^{\+} are both infinite, since the length of the strings in these sets is not limited in any way. Therefore, a language =L= can be defined as a subset of \Sigma^{\*}, where the corresponding alphabet \Sigma contains all different characters used in =L=. This means, that two languages =L1= and =L2= might share the same alphabet \Sigma, but each could contain a subset of \Sigma^{*} such that
\begin{equation}
L_{1} \cap L_{2} = \varnothing.
\end{equation}
Since languages are just sets by definition, as they are subsets of the set \Sigma^{\*}, the operations union, intersection and difference are immediately applicable. Concatenating two languages =L1= and =L2= will result in every element of =L1= being concatenated with every element of =L2=. Another important definition is $L^n$, which is the language $L$ concatenated with itself $n$ times. Following special cases
\begin{equation}
L^0 = {\lambda}\\
L^1 = L
\end{equation}
the star-closure
\begin{equation}
L^* = L^0 \cup L^1 \cup L^2 ...
\end{equation}
and the positive closure
\begin{equation}
L^+ = L^1 \cup L^2 ...
\end{equation}
are applicable to every language =L=.[cite:see @books/PLinz/fla-2017 p. 19-20]
With this solid definition of languages, we are able to define regular expressions and regular grammars in the following sections.

*** Regular Expressions
When describing a language with regular expressions in a formal way, a special notation is used. This notation consists of words created from characters of an alphabet \Sigma, parentheses =()= and the operators =+=, =\cdot= and =*=. The operators =+=, =\cdot= and =*= signify different set operations on a language =L=. =+= is denotes the union, =\cdot= denotes concatenation and =*= denotes the star-closure.[cite:see @books/PLinz/fla-2017 p. 74]
We can define regular expressions in a formal matter the following way:
#+begin_quote
Let \Sigma be a given alphabet. Then
1. \varnothing, \lambda, and $a \in \Sigma$ are all regular expressions. These are called *primitive regular expressions*.
2. If $r_1$ and $r_2$ are regular expressions, so are $r_1 + r_2$, $r_1 \cdot r_2$, $r_1^{\*}$ and $(r_1)$.
3. A string is a regular expression if and only if it can be derived from the primitive regular expressions by a finite number of applications of the rules in (2).[cite:see @books/PLinz/fla-2017 p. 74]
#+end_quote
The some small problems arise, when many of the operators defined above are used in conjunction. To further elaborate this, let's consider the regular expression $a \cdot b + c$. We now have two valid options to represent this regular expression: $r_1 = a \cdot b$ and $r_2 = c$ which results in the language $L(a \cdot b + c)= {ab,c}$ or $r_1=a$ and $r_2=b+c$ which results in $L(a \cdot b + c)={ab,ac}$. To prevent such ambiguity, we could either parenthesize all expressions, but this would be hard to read. Therefore, a precedence is given to each operator, which ensures that each one is evaluated in the correct order. This means, that =*= star-closure precedes =\cdot= concatenation, which precedes =+= union. Furthermore, the operator for concatenation can be omitted and we can rewrite $r_1 \cdot r_2$ to $r_1 r_2$.[cite:see @books/PLinz/fla-2017 p. 76]
With this formal definition of regular grammars and their operators, we can now define the different operators used in modern pattern matchers for regular expressions, as they differ from their formal counterparts. In modern regular expression syntax, the operator for concatenation =\cdot= is always omitted. The union operator =+= can be replaced with the logical OR =|=, since it also creates a set consisting of the elements on each side of =|= where one of them can be selected to match. The =+= operator now signifies the positive closure, while the =*= operator does not change and still signifies the star-closure.

*** Regular Grammars
Regular grammars are another way to represent languages and language families, that can be represented with an automaton. Two types of regular grammars are possible.[cite:see @books/PLinz/fla-2017 p. 92]
#+begin_quote
A grammar $G=(V,T,S,P)$ is said to be *right-linear* if all productions are of the form
\begin{equation}
A \rightarrow xB,\\
A \rightarrow x,
\end{equation}
where $A,B \in V$, and $x \in T_{\*}$. A grammar is said to be *left-linear* if all productions are of the form
\begin{equation}
A \rightarrow Bx,
\end{equation}
or
\begin{equation}
A \rightarrow x.
\end{equation}
A *regular grammar* is one that is either right-linear or left-linear.[cite:see @books/PLinz/fla-2017 p. 92]
#+end_quote
A defining characteristic of regular grammars is, that at most one variable can appear on the right side of any production rule and the variable must consistently be placed on the far right or the far left on the right side of the production rule.[cite:see @books/PLinz/fla-2017 p. 92]
In the following chapters, the grammars used will strongly adhere to the EBNF notation as it is defined in ISO/IEC 14977 from 1996. This means, that instead of =\rightarrow=, === will be used to define a production rule and all possible options for said rule are separated with =|=. For our purposes, the empty string will be represented by \epsilon instead of \lambda, but \lambda will be used in formal definitions. Consider following example:
\begin{equation}
A \rightarrow xA,\\
A \rightarrow xB,\\
B \rightarrow yB,\\
B \rightarrow \lambda,
\end{equation}
can be rewritten to
\begin{equation}
A = xA|xB;\\
B = yb|\epsilon;
\end{equation}
which now adheres to the afformentioned EBNF notation. Note, that each rule is seperated by a semicolon as well, which further ties into the definition of EBNF.

*** Genetic Programming
After defining the basis for the representation of languages with regular expressions and regular grammars, we can now move on to the last part needed for this project, *genetic programming*, which itself is based on *genetic algorithms*.
#+begin_quote
The /genetic algorithm/ is a highly parallel mathematical algorithm that transforms a set (/population/) of individual mathematical objects [...], each with an associated /fitness/ value, into a new population (i.e., the next /generation/) using operations patterned after the Darwinian principle of reproduction and survival of the fittest and after naturally occurring genetic operations [...].[cite:see @books/JRKoza/gp1-1993 p. 18]
#+end_quote

**** Function and terminal set
In genetic programming, like in every other adaptive system, at least one structure is altered and adapted during learning. What makes genetic programming special, is, that instead of a single point, all individual elements of the search space are being adapted, instead of a single element. This means, that genetic methods can search hundreds or thousands of points in the search space in parallel. The individual structures that are processed by genetic programming are hierarchical computer programs. The set of possible individual structures is given by all possible compositions of elements from a function set =F= and a terminal set =T=. Each function of =F= takes a specified number of arguments, which signifies a functions /arity/. These functions can be arithmetic operations, boolean operators or any other function that is domain-specific. Terminals can either be variable atoms, like the state of a system, or constant values, like the number =3= or the boolean value =true=.[cite:@books/JRKoza/gp1-1993 p. 79-80]
A very important property of each function of =F= is /closure/. Closure means, that the output of any function in $f_1 \in F$, may possibly be used as input for any other function $f_2 \in F$. This also means, that every function that could produce an error, like $\div$ when dividing by 0, has to handle such cases in a gracious way that produces a resonable alternative result. In this case, a resonable output could be 1, when dividing by 0.[cite:@books/JRKoza/gp1-1993 p. 81-82]
The other very important property for functions =F= and terminals =T= for a given problem is /sufficiency/. Sufficiency means, that the set of all possible individual structures using =F= and =T= can yield a solution for the given problem. It is therefore necessary to identify the functions and terminals that have this sufficient power of expression, even if this task might sometimes be impossible and the number of variables has to be restricted to a set that comes close enough to a perfect solution.[cite:@books/JRKoza/gp1-1993 p. 86-87]

**** Initial Structure
The initial structure in genetic programming contains randomly generated individuals which form the initial population. Each individual is a rooted, point-labeled tree with ordered branches. The root element of an individual is always a function and for each argument the function needs a new branch of the tree is created. For each of this branches, we now select a random value from the union of function set and terminal set $v \in F \cup T$. If =v= is a terminal, it is added to this branch as a leaf node and if =v= is a function, the amount of branches corresponding to the arity of =v= is created from =v=. This process repeats recursively until every branch is satisfied and ends in a terminal as leaf node. Two basic methods to create the initial population of a tree exist. When generating an individual with the "full" method, only items from the function set are selected until the specified maximum tree depth. Afterwards all remaining nodes are populated with items from the terminal set, resulting in trees that always reach the maximum depth and are "bushy", due to the exclusive use of functions until the maximum depth. To generate a tree with the "grow" method, a random item is chosen from the union of functions and terminals. If a terminal is being chosen, it is treated as a leaf and no further nodes are added. If a function was selected, more nodes are being added according to the functions arity. Therefore the "grow" method corresponds to the initial explanation on creating an individual. In contrast to the "full" method, trees can be significantly shorter than those created by the "full" method. To create an initial population with varied shapes, "ramped-half-and-half" combines the full and grow method. Using ramped-half-and-half, trees are generated in the range of depth 2 and the maximum tree depth and creation switches between the full and grow methods. This results in half the populatio created with the full, and half the population created with the grow method.[cite:@books/JRKoza/gp1-1993 p. 91-93]

**** Fitness
The fitness of an individual in nature is the probability of survival until the age of reproduction, so that offspring can be created. In the artificial world of algorithms, fitness defines how different operations are applied to our structures. The common approach for doing this requires us to assign a fitness value to each individual of our population. To calculate our fitness value, we usually evaluate a set of fitness cases with a given individual and determine how well this individual did perform the specific task.[cite:@books/JRKoza/gp1-1993 p. 94-95]

***** f1-score
The f1-score consists of two basic measures, /precision/ and /recall/. Precision is defined as the fraction of relevant items in relation to the total of retrieved items.[cite:@journals/IKlampanos/ir-2009 p. 154-155]
In our case, we can take a look at our fitness cases and determine how many of them returned a "truthy" value. Furthermore, we need two sets of fitness cases, "true" cases and "wrong" cases, where "true" cases should return true when evaluated with an individual and "false" cases should return false.
| Evaluation result $rightarrow$ |                   |                   |
| Expected result $downarrow$    | positive          | negative          |
|--------------------------------+-------------------+-------------------|
| positive                       | true-positive TP  | false-negative FN |
| negative                       | false-positive FP | true-negative TN  |
\begin{equation}
\text{Precision}=\frac{text{#(relevant items retrieved)}}{text{#(retrieved items)}}=P(text{relevant}|text{retrieved})
\end{equation}
To account for the result of our fitness cases, precision can also be defined as
\begin{equation}
\text{Precision}=\frac{text{#TP}}{text{#TP}+text{#FP}}.
\end{equation}
Recall on the other hand is the amount of relevant documents amongst those that have been retrieved.
\begin{equation}
\text{Recall}=\frac{text{#(relevant items retrieved)}}{text{#(relevant items)}}=P(text{retrieved}|text{relevant})
\end{equation}
This results in following formula for recall
\begin{equation}
\text{Precision}=\frac{text{#TP}}{text{#TP}+text{#FN}}.
\end{equation}
[cite:@journals/IKlampanos/ir-2009 p. 155]
We can now combine precision and recall to trade them off against each other, so that we get a high amount true positives while tolerating only a small amount of false positives. A good measure which achieves this is the /F measure/, which results in the /f1-score/, if precision and recall are evenly weighted.
\begin{equation}
F_{\beta = 1} = 2 \times \frac{P \times R}{P + R}
\end{equation}
This results in values between 0 and 1, where 1 signifies a perfect score.[cite:@journals/IKlampanos/ir-2009 p. 156]

**** Reproduction
Reproduction lies at the basis of Darwinian natural selection and it is asexual, as it only operates on one individual. In reproduction, a single individual is selected from a population and is copied as-is to the next generation. There exist a few ways to select an individual, but in our case, /tournament selection/ is being used. In order to apply tournament selection, a specified amount of individuals is selected from the population. Typically two individuals are being selected and the one with the better fitness value will be chosen.[cite:@books/JRKoza/gp1-1993 p. 99-100]

**** Crossover
The crossover operation works on two individuals at a time by combining parts taken from each parent, therefore it is a sexual operation. Crossover always uses two individuals and produces two individuals as offspring. In this project, tournament selection is being used to select both parent individuals from a generation, however, like reproduction, other methods to select an individual can be applied as well. In each of the parent individuals, a crossover point is determined at random and the subtree of parent =B= replaces the subtree at the crossover point of parent =A=. The same happens for parent =B=, where the selected subtree is replaced by the subtree in parent =A=. It is important to note, that this operation is non-destructive and preserves =A= and =B= without altering them. The newly created offspring has to pass a size check, which is done via maximum tree depth. If one or both of the offspring are too deep, an equal amount of parent programs will reproduce into the next generation. This mechanism exists to prevent a high increase in computation time due to a few large individuals.[cite:@books/JRKoza/gp1-1993 p. 101-104]

**** Mutation
Mutation introduces random changes in an individual. For this to happen, a single individual is selected from the population with tournament selection. To execute the mutation a random point within the individual is chosen and the whole subtree is replaced with a new randomly created one. This new subtree is created with the same method, as the initial population and the maximum depth of this subtree is typically the same as the one used to create individuals of the initial population as well. However, it is important to note, that mutation should play a minor role in genetic programming as it is relatvely unimportant.[cite:@books/JRKoza/gp1-1993 p. 105-106]
This project also includes a special form of mutation, /subtree deletion/, where a whole subtree, which has been selected at random, is replaced with a terminal. This can be used if each generation introduces larger and larger individuals and it aims to reduce this bloat.

**** Termination criterion
Even though genetic programming could evolve endlessely like nature, we are more interested in retrieving the best result if a fixed termination criterion is met or a predetermined number of generations has been exhausted. The termination criterion is mostly a problem-specific success predicate, which often requires a solution that solves the given problem with 100%-correctness. If we are unable to determine a correct solution, we can either select a lower criterion, or, if that proves impossible as well, we can omit it completely and merely analyze the results after the algorithm was run for =G= generations.[cite:@books/JRKoza/gp1-1993 p. 113]

**** Result designation
In result designation we simply present the best individual of the run. For our purposes, this is either the individual that satisfied the termination criterion or the best-of-run after running =G= generations. This best-of-run individual is only based on caching the best-so-far individual after each generation.[cite:@books/JRKoza/gp1-1993 p. 113]

** Methodology
This chapter will focus on the implementation details of a genetic programming system for regular grammars as well as regular expressions. The first part will focus on the general implementation of the algorithm and the provided utility functions, while the second and third part will resolve around the implementation of regular expressions and regular grammars respectively.

*** Generic kernel
The generic kernel of the genetic programming system is responsible for all parts that are problem independent. The tasks of the kernel are generating an initial population, selecting individual elements and applying the operations reproduction, crossover and mutation, as well as keeping track of the best-of-run individual and presenting the final result. For all this steps it uses a supplied terminal and function set as well as problem specific functions for fitness measurement and the termination criterion, which have to be supplied when starting a run. This generic kernel is mainly a port of John R. Koza's LISP implementation which can be found in [cite:Appendix C @books/JRKoza/gp1-1993 p. 735-755]. Only minor tweaks were added to this algorithm in order to better solve the problems at hand, but this tweaks are applicable to other problems as well, as they are fairly generic. The first addition is the possibility to use an optional optimizer function, which can be used to simplify a given individual before adding it to a population. If no optimizer is needed, the ~identity~ function should be supplied, as it simply returns the given individual. Another addition is the requirement to implement a pretty print function for each problem, which should give each specific implementation the power to present the results for each generation in a meaningful and readable way. The last addition is the special mutation operation of subtree deletion, which replaces a randomly selected mutation point in the individual with a terminal value.
There are also some functions that have been omitted from the original code, like the ~normalize-fitness-of-population~ function, as this is not needed when determining the fitness with the f1-score. Fitness proportionate selection and fitness proportionate selection with over selection have not been implemented as well, since only tournament selection will be used. Some parts of the algorithm have been adapted to better suit the Clojure way of coding and usage experience. This is the reason why instead of supplying a function which sets all necessary parameters to run the genetic programming system, a config struct and the requirement to provide parameters directly as input was chosen, as it provided an easier environment to experiment with different parameters in Clojures interactive environment, the REPL (Read-Evaluate-Print-Loop). In general, the implementation should not differ to much from the original in terms of functionality, while offering the possibility to easily extend it to include the omitted parts.

**** Utility functions
A small set of utility functions has been implemented, which make working with the data structures used within the genetic programming system easier. This should provide a small overview over these functions and their usage.

***** Retrieving terminals from word maps
The primary datastructure provided to the genetic programming system is a map of valid and invalid words.
#+begin_src clojure
{:valid-words ["aa" "a" "aab" "" ...]
 :invalid-words ["b" "bb" "bba" "ba" ...]}
#+end_src
However, this list of words cannot be processed by the genetic programming system, because it only works on functions and terminals. Therefore, the function ~get-terminals-from-map~ extracts the unique set of terminals by first extracting all words of the map into a single vector with ~get-words-from-map~ and then retrieving the terminals with ~get-terminals~. Get terminals concatenates all elements of the provided collection and then converts the string to a set, which only preserves unique characters. These unique characters are then returned as a vector of strings.

***** in?
Since Clojure has no function in its core library, that checks if an element is part of a collection, this function provides a shorthand for the common implementation of this functionality.

***** map-tree
~map-tree~ is an extension of the ~map~ function, which operates on a tree instead of a collection. It traverses each node while applying a function f and returns the result of this operation.

***** f1-score
~f1-score~ is the implementation of the f1-score measure, as previously defined. It recieves a map of evaluated valid and invalid words together with a function that is able to transform this map into the amount of true and false values. This count is then used to calculate the f1-score according to the predefined formulat.

***** median
The ~median~ function recieves a collection of integers or floating point numbers and calculates the median by sorting the collection and either selecting the value in the middle or by calculating the average of the two values in the middle.

***** generate-word-map
This function takes two regular expressions, one which matches words that are valid and one which only matches invalid words. It then applies these regular expressions to a generator and generates the specified amount of strings. It is also possible to supply a map with previously created words, where it will attempt to create additional words until the specified amount is satisfied.

***** boolean-reducer
The ~boolean-reducer~ is one of the functions that can be supplied to ~f1-score~ in order to retrieve the true and false values from a map. ~k1~ and ~k2~ are used, so that the caller of this function is able to specify the keys of the created map. A value is counted as true, if it is truthy by Clojures's standards.

*** Regular expressions
The following section describing the implementation of regular expressions for genetic programming is divided in two parts. The first part will briefly introduce the used operations of regular expressions, while the following parts will focus on the implementation details of required functions to efficiently run the genetic programming system.

**** Definitions
As for any problem presented to a genetic programming system, terminal and function set provide the basis for successful evolution. The terminal set for regular expressions is rather simple, as it should consist of a unique set of characters which the different meta symbols of regular expressions are applied to. Regex allows for many different meta symbols, like operators, scopes and groups, but a small subset will be sufficient for our purposes and serve as a function set.
- =*=: capture the preceding token or group 0 to \infin times.
- =+=: captures the preceding token or group 1 to \infin times.
- =?=: captures the preceding token 0 or 1 times.
- =|=: logical OR, combines preceding and following token.
- =&=: logical AND, combines preceding and following token.

  As all of these operators are supplied to the genetic programming system as functions, an arity has to be assigned to each of them. This can quite easily be achieved, as each of these operators either exclusively influences the preceding token, or the following token as well. =*=, =+= and =?= are 1-ary (unary) and =|= and =&= can be defined as 2-ary (binary) functions.

**** Fitness
Before determining the best individual of a population, the performance of each individual has to be evaluated. To evaluate our regular expressions, constructing a regex string from our trees is necessary. Every predefined function must be resolved correctly, in order to produce the desired regular expression and to ensure the validity of the result.

***** =&= function
The =&= function is the simplest to resolve, since the =&= itself is not present in regex syntax and it merely combines both of it's parameters into a single string.
#+begin_example
("&" "a" "b") => "(ab)"
#+end_example
As already shown above, all functions will be wraped as groups, to ensure that the expected result is being produced.

***** =|= function
The =|= function is a little bit more difficult to process, since the =|= symbol itself has to be placed between it's two arguments.
#+begin_example
("|" "a" "b") => "(a|b)"
#+end_example
Grouping is applied as well to ensure consistency.

***** Operator functions =*=, =+= and =?=
All operator functions =*=, =+= and =?= can be implemented the same. Each of these functions has one argument and after all processing is applied, the operator should be after the argument.
#+begin_example
("*" "a") => "a*"
("+" "b") => "b+"
("?" ("&" "a" "b")) => "(ab)?"
#+end_example
For operators, no grouping is applied, since it isn't needed for single arguments and structures that combine two tokens like =&= already apply grouping.

**** Fitness (continued)
The conversion according to these rules is applied in ~tree->regex-str~, which uses the rewrite functionality of ~meander~ together with the following rules to create the desired structure, which is a vector of strings, which only has to be flattened and joined together to create a valid regular expression.
#+begin_src clojure
   [:* ?t] [?t "*"]
   [:+ ?t] [?t "+"]
   [:? ?t] [?t "?"]
   [:& ?a ?b] ["(?:" ?a ?b ")"]
   [:| ?a ?b] ["(?:" ?a "|" ?b ")"]
#+end_src
~rewrite~ rules can be defined in a style that very closely resembles logic programming languages like Prolog. A rule matches the left side of its definition and returns the right hand side as result. Variables start with ~?~ and are substituted accordingly.

The resulting regular expression is applied to each of the words in the word map by matching the pattern on each string.
#+begin_src clojure
(let [pattern (re-pattern regex-str)]
    (map #(-> pattern (re-matches %)) string-list))
#+end_src
This results in list of values that contains either the match or ~nil~ for each string. This result is the supplied to the ~f1-score~ function together with the ~boolean-reducer~, which returns true for a match and false for ~nil~. The fitness function finally returns the score determined by ~f1-score~.

**** Optimizing regular expressions
 A big caveat of generating regular expressions with genetic programming is, that it can result in expressions with very poor performance. In order to preserve computation time, some optimizations based on rules have been implemented. The first performance problem is related to groups, as simple round braces =()= will create capture groups which will always store the matched value separately. Since we don't need any captured value and are only interested in the final match, we can safely ignore such capture groups. Therefore, we turn all automatically created groups into non-capturing groups =(?:)= which will not store their matched values.
 The next step to optimize regular expressions is to reduce duplicates of the operators =*=, =+= and =?=. Consider the following regular expressions:
 #+begin_example
 ((a*)*)*
 (a+)+
 (((a?)?)?)?
 #+end_example
 Nesting the same operator in this way does not improve the regular expression. Instead, it reduces readability and introduces unnecessary complexity.
 To solve this problem, all trees generated for regular expressions will be preprocessed by using the ~rewrite~ functionality of ~meander~.
 Following rules are used to rewrite the regular expressions from above:
 #+begin_src clojure
   [:* [:* ?t]] [:* ?t]
   [:+ [:+ ?t]] [:+ ?t]
   [:? [:? ?t]] [:? ?t]
 #+end_src
 These rules will unnest the corresponding tree structure and transform it into our desired result:
 #+begin_example
 [:* [:* "a"]] -> [:* "a"]
 #+end_example

 The next group of patterns that can easily be simplified, are those that result in a 0-to-n match =*=. These patterns are any combination of =+= and =?= and any combination of =+= and =?= with =*=.
 #+begin_src clojure
   [:+ [:? ?t]] [:* ?t]
   [:? [:+ ?t]] [:* ?t]
   [:+ [:* ?t]] [:* ?t]
   [:* [:+ ?t]] [:* ?t]
   [:? [:* ?t]] [:* ?t]
   [:* [:? ?t]] [:* ?t]
 #+end_src
 The same simplification rules can be applied when the same value is on both sides of =&= and =|= like this
 #+begin_src clojure
   [:& [:+ ?t] [:? ?t]] [:* ?t]
   [:| [:+ ?t] [:* ?t]] [:* ?t]
    ...
 #+end_src
 Some expressions combined with =&= result in the operator =+= and can therefore be extracted.
 #+begin_src clojure
   [:& [:+ ?t] ?t] [:+ ?t]
   [:& ?t [:+ ?t]] [:+ ?t]
   [:& [:* ?t] ?t] [:+ ?t]
   [:& ?t [:* ?t]] [:+ ?t]
 #+end_src
 The we can extract an expression with an operator as well, if the operator is only applied to one side of an =|=.
 #+begin_src clojure
   [:| [?o ?t] ?t] [?o ?t]
   [:| ?t [?o ?t]] [?o ?t]
 #+end_src
 If operators are outside of =&= or =|=, we can pull them into the expression to apply the rules from above to the tokens of =&= and =|=.
 #+begin_src clojure
    [:* [?o ?a ?b]] [?o [:* ?a] [:* ?b]]
    [:+ [?o ?a ?b]] [?o [:+ ?a] [:+ ?b]]
    [:? [?o ?a ?b]] [?o [:? ?a] [:? ?b]]
 #+end_src
 We can also simplify logical expressions according to distributivity.
 #+begin_src clojure
    [:& [:| ?a ?b] [:| ?c ?a]] [:| ?a [:& ?b ?c]]
    [:| [:& ?a ?b] [:& ?a ?c]] [:& ?a [:| ?b ?c]]
 #+end_src
 As a last step, we can try to extract operators from =&= and =|= if they are applied to both contained tokens.
 #+begin_src clojure
    [:& [?o ?a] [?o ?b]] [?o [:& ?a ?b]]
    [:| [?o ?a] [?o ?b]] [?o [:| ?a ?b]]
 #+end_src

**** Termination predicate
The termination predicate determines at which point a single individual, namely the best of generation, is sufficient enough to solve a given problem. For regular expressions, following termination predicate has been defined.
#+begin_src clojure
  (when (>= (:score best-of-gen) 0.99)
    (->> population
         (map #(:size %))
         (utils/median)
         (<= (:size best-of-gen))))
#+end_src clojure

We first determine if a given best of generation individual achies a score =S= of at least 0.99. The number 0.99 was chosen, as the floating point operations done in ~f1-score~ can return values smaller than 1 due to imprecise calculations. A score =S= of 0.99 or better is not sufficient, since we want an individual that is as small as possible as well. Therefore the median of the size of the population is calculated, where the current best of generation individual should have a size that is lower or equal to the median.

*** Regular grammars
The last section of this chapter describes how the creation of regular grammars has been implemented for genetic programming. The first part contains some general definitions, while the following parts will explain the implementation in greater detail.

**** Definitions
In contrast to regular expressions, where the terminal set consisted of a set of unique characters, a different approach is used for regular grammars. Where regular expressions used single characters as terminals, production rules will be used as terminals for regular grammars. A production rule is implemented as a map with four entries. A non-terminal, which is on the left hand side of a rule, a terminal, which is created or matched by this rule and a reference, which is used to link to other production rules and for recursion.
#+begin_src clojure
{:non-terminal :A
 :terminal "a"
 :reference :B}
#+end_src
Furthermore, two functions are provided to the function set, === and =&=.
- === returns the rule it has as parameter
- =&= returns the two rules in a single string
The arity for these pseudo-functions is unary for === and binary for =&=.

**** Terminal set
Since the terminal set for regular grammars consists of production rules, some helper functions are needed to create all combinations of a supplied number of terminals and non-terminals.

***** generate-non-terminals
This function aids in the flexible creation of a list of non-terminals as characters. It produces the specified amount of non-terminals, but not more than the latin alphabet.

***** create-rules-for-refs
Uses a given non-terminal and terminal to create a collection of rules with ~create-rule~, where each rule references one of the element in ~non-terminal-refs~.

***** generate-rule-set
~generate-rule-set~ iterates over a set of non-terminals and a set of terminals to an exhaustive list of all possible combinations of rules. First, a rule with a given terminal without a reference and a rule which results in \epsilon, the empty set, is created. Then all possible combinations with the other non-terminals is created with ~create-rules-for-refs~. All of the created rules are combined and appended to the result list. If the terminal set is empty and there are still non-terminals in the non-terminal set, we supply a full set of the same terminals again.

***** create-terminal-set
This function receives a non-terminal set and a word map, retrieves all unique characters from the word map and generates all possible rules with ~generate-rule-set~.

**** Fitness



** Experiments
Run and result of program
First, show the optimal solutions, based on the converted dfa's from AAlpy and the ICANN paper
Then show the results of executing the program with short explanations




**** Control parameters
In order to create comparable results across all evalulated problems, the same set of control parameters is used to evaluate each of them. As there exist some parameters that have already been tried and tested, the control parameters suggested by John R. Koza will be used except for those where additional information has been provided.
- The population size is 500 individuals.
- The maximum number of generations is 50, where the initial generation is used as generation 0, so that there are 51 generations in total.
- The probability of mutation is 0.01. This is different from John R. Koza's recommendations, because we want to allow the usage of subtree deletion.
- The probability of reproduction is 0.09, which allows for 1% of the new generation to be created by mutation.
- The probability of crossover is 0.9.
- When executing crossover, 90% of the time a function will be selected as crossover point an 10% of the time any point can be selected for crossover.
- The maximum depth for crossover offspring is 17.
- The maximum depth for the individual generation as well as mutation is 6.
- The method to generate the initial population is ramped half-and-half.
- The method of mutation is subtree deletion.
- The method of selection in all cases is tournament selection.
- The fitness measure is created using the f1-score.
  [cite:@books/JRKoza/gp1-1993 p. 114]

** Conclusion
What is the conclusion i.e. is it feasible to generate regular grammars this way?
** Acknowledgments
Thanks to Ben Sless from the Clojurians Slack!

**** Examples for REGEX
This section provides small example rulesets and their possible graph based representation using regular expressions.

***** Recursive rules
The following regular grammar ruleset produces only words containing the letter =a=. A word consists of at least one a.

- V_{T} = {_a_}
- V_{N} = {S, A}

- S \rightarrow A
- A \rightarrow _a_ A
- A \rightarrow _a_

The graph representation of a regular grammar for this ruleset could look like this:
#+begin_src clojure
("+" "a") ;; produces the regular expression '(a)+'
#+end_src

Another possible solution could be:
#+begin_src clojure
("&" "a" ("*" "a")) ;; produces the regular expression 'a(a)*'
#+end_src

***** Circular rules
The next ruleset defines a regular grammar, that produces any number of =ab=. The empty word \epsilon is allowed as well.

- V_{N} = {_a_, _b_}
- V_{T} = {S, A, B}

- S = A
- A = aB
- A = \epsilon
- B = bA

Following graph could represent this ruleset:
#+begin_src clojure
;; produces the regular expression '(ab)*'
("*"
  ("&" "a" "b"))
#+end_src

***** Exit conditions
The previous ruleset should now be expanded, so that at least one pair =ab= should be produced.

- V_{N} = {_a_, _b_}
- V_{T} = {S, A, B, C}

- S \rightarrow A
- A \rightarrow aB
- A \rightarrow aC
- B \rightarrow bA
- C \rightarrow b

This could yield the following graph:
#+begin_src clojure
("+"
  ("&" "a" "b"))
#+end_src

**** Difficulties
Representing regular grammars with regular expressions was very straightforward so far, but regular expressions come with their own difficulties. For better display, following rules are given:

- V_{N} = {_a_, _b_}
- V_{T} = {S, A, B}

- S \rightarrow A
- A \rightarrow aB
- A \rightarrow \epsilon
- B \rightarrow bA
- B \rightarrow \epsilon

These rules are very similar to our previous examples, but now any alteration of =a= and =b= is allowed. For example =a= and =aba= are valid as well. This combination can be represented using regex, but the expression is already harder to read:
#+begin_src clojure
;; produces '((ab)*)a?'
("&"
  ("*"
    ("&" "a" "b"))
  ("?" "a"))
#+end_src
